---
title: "testing effort layer weighting"
author: "Thomas MM"
date: "6/22/2021"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)

library(tidyverse)
library(doParallel)
library(foreach)
library(raster)
library(viridis)
library(lubridate)

source("../../Scripts/modules/filter_distance.R")

# read in records data
dfm <- read_csv("../../Data/species_data/moth/DayFlyingMoths_EastNorths_no_duplicates.csv")
dfm


```

## get up to date species data 

```{r species_data, warning=F, echo = F, message = F, include = F}

# model = c('rf', 'lr', 'gam')
taxa = 'moth'
pseudoabs = 'PA_thinned_10000nAbs'


model_locs <- paste0('/data-s3/thoval/sdm_outputs/', taxa, '/combined_model_outputs/', pseudoabs)

names <- gsub(pattern = '_PA_thinned_10000nAbs_weightedmeanensemble.grd', replacement = '', 
              list.files(model_locs, 
                         pattern='_weightedmeanensemble.grd'))
# names

# sdm outputs for each species
species_stack <- list()

# error outputs
error_out <- list()

for(i in 1:length(names)){
  
  print(names[i])
  
  # initiate model list within for loop so that it gets replaced when starting a new species
  # otherwise we might get some weird overlaps
  
  # mean predictions
  mp <- list.files(model_locs, 
                   pattern = paste0(names[i], "_", pseudoabs, "_weightedmeanensemble.grd"),
                   full.names = TRUE)
  
  mod_preds <- raster::stack(mp)
  names(mod_preds) <- paste0(names[i], '_mean_pred')
  
  
  
  # quantile range
  qr <- list.files(model_locs, 
                   pattern = paste0(names[i], "_", pseudoabs, "_rangeensemblequantiles.grd"),
                   full.names = TRUE)
  
  qrnge <- raster::stack(qr)
  names(qrnge) <- paste0(names[i], '_quantile_range')
  
  species_stack[[i]] <- raster::stack(mod_preds, qrnge)
  
}


```

## location to crop to for all cropping

```{r loc_dist}

# location = c(-0.385843, 51.289680) # bookham common
location = c(-2.860564, 56.014902) # aberlady
# location = c(-1.503560, 54.141389) # sharow
# location = c(-2.247979, 50.632404) # lulworth cove
# location = c(-1.461817, 54.440435) # great smeaton
# location = c(-1.110557, 51.602436) # wallingford


distance = 5000

```

## Crop prediction and variation to desired location 

```{r crop, warning = F, include = F}

registerDoParallel(7)

# out_cropped <- list()
system.time(
  out_cropped <- foreach(s = 1:length(species_stack)) %dopar% {
    
    print(s)
    
    sp <- species_stack[[s]]
    
    
    # crop the prediction
    crop_pred <- filter_distance(obj = subset(sp, grep(pattern = 'mean_pred',
                                                       names(sp))),
                                 method = 'buffer',
                                 distance = distance,
                                 location = location)
    
    # crop the error
    crop_err <- filter_distance(obj = subset(sp, grep(pattern = 'quantile_range',
                                                      names(sp))),
                                method = 'buffer',
                                distance = distance,
                                location = location)
    
    if(length(names(crop_pred))>1){
      # get the mean
      m_pred_av <- calc(crop_pred,
                        mean, na.rm = T)
      names(m_pred_av) <- 'predictions'
      
      
      m_quant_av <- calc(crop_err,
                         mean, na.rm = T)
      names(m_quant_av) <- 'error'
    } else {
      
      m_pred_av <- crop_pred
      m_quant_av <- crop_err
      
    }
    
    out_rasts <- list(m_pred_av, m_quant_av)
    names(out_rasts) <- c('predictions', 'quantile_var')
    
    return(out_rasts)
    
  }
)

registerDoSEQ()

names(out_cropped) <- names


# get the cropped probability of presence
preds <- stack(lapply(1:length(out_cropped), FUN = function(x) out_cropped[[x]]$predictions))
names(preds) <- names(out_cropped)

# get the cropped variation
var <- stack(lapply(1:length(out_cropped), FUN = function(x) out_cropped[[x]]$quantile_var))
names(var) <- names(out_cropped)


```

Going to use mean variation across all species for the decide score layer

```{r}

dec_crop <- mean(var)
plot(dec_crop)

```


## crop species data frame to region of interest


```{r}

dfm <- dfm %>% 
  dplyr::select(lon, lat, date, year = yr, species = sp_n, common_name=com_n)
head(dfm)

# convert records to an sf object
st_dfm <- st_as_sf(dfm, coords = c('lon', 'lat'), crs = 27700)

# get count of all records as a raster
# crop
records <- filter_distance(obj = st_dfm,
                           distance = distance,
                           location = location, 
                           method  = 'buffer')

plot(dec_crop)
points(x = dfm$lon, y = dfm$lat, col = 'red')
plot(st_geometry(records), add = T, pch = 20)



```

## function to count the number of records that falls in each cell of the raster

We need a way of downweighting cells that have lots of sightings in them. With weight_by_time = FALSE in the function below, it returns a layer of the number of records in each grid cell, irrespective of the year in which those records were made.

But we also want to downweight cells that have recent records in them. 

Below are a series of steps that I have taken to try and weight by timings:

### first

I have done it so that all the unique years in the records dataframe are ranked oldest to newest, and used this rank to divide the number of records in a cell. Meaning that the oldest records in a cell will be divided by 1; the newest by the number of unique records. 

The problem is: If we consider two cells that were recorded in the oldest year, e.g. 2010, they are both divided by 1. But if one cell has 2 records and the other 10, then the one with more records will be upweighted relative to the one with fewer records. So, what do we do?

### second

Other option is take the number of records in a cell and the number of years that have passed since those records were made. Divide the number of years since the last record by the number of records and use that as a score. The problem with this is that the records across all years are considered important in the 'score' given to a cell; whereas, we are probably most interested in only the most recent records. I.e. we don't care that a cell had 1 record in 2011 if it had 10 in 2019.

### third 

Other option is to take the number of years since the most recent record and divide it by the number of records in that year. This means that grid cells with few records from a long time ago would be the most upweighted. Grid cells visited recently with most records would be the most downweighted. The problem is that this treats records in the same way as the number of years since the last record and means that, for e.g., 1 record in 2013 [(2021-2013)/1=8] has a higher weighting than 5 records in 2011 [(2021-2011)/5=2]. Which we don't want. 

### fourth and final

So, we probably want time since the last record, irrespective of the number of visits. So, just time since the last record as the way of creating a weighting to affect the decide score. I have done 1/(number of years since last record), so that small numbers are places to go (i.e. with few records) and large numbers are places to avoid (i.e. lots of records/most recent records). This is so that the output of the 'weighted by time' option in the function, is consistent with outputting just a count of the number of records.

This function can now produce two outputs. The first is just the number of records in a cell across all years. The second, is 1 divided by the number of years since the most recent record. In both cases, larger numbers are bad (i.e. most/most recent records) and smaller numbers are better (i.e. fewest/oldest records).

This might not work if the data and the raster are at different resolutions. Slow on large rasters. Might be possible to do this in package sf rather than relying on raster. Only possible if the coords of the data frame and raster are the same.


```{r}

####    Function
# not sure if this will work with raster and sf object with different resolutions
count_records <- function(records_df, # sf object of record counts
                          template_raster, # raster that you want the counts to be aggregated by
                          Coords = 27700, 
                          weight_by_time = TRUE)
{
  
  # Get the counts of records per cell and store as data frame
  record_counts <- records_df %>% 
    mutate(lon = unlist(map(records_df$geometry, 1)),
           lat = unlist(map(records_df$geometry, 2)))  %>% 
    as.data.frame() %>% 
    mutate(geometry = NULL)
  
  if(weight_by_time){
    
    # # get the years ranked    
    # yr_rnk <- data.frame(yr = unique(record_counts$year), rnk = rank(unique(record_counts$year)))
    # 
    # # bind to records data.frame
    # record_counts$rnk <- yr_rnk$rnk[match(record_counts$year, yr_rnk$yr)]
    
    record_weights <- record_counts %>% 
      group_by(lon, lat) %>% 
      dplyr::summarise(last_rec = max(year), # what is the most recent year?
                       yrs_since_last_rec = (year(Sys.Date())-last_rec), # number of years since the last record in a grid cell
                       score = 1/yrs_since_last_rec) %>% # get the big values small and vice-versa, so that larger numbers are 'bad' and smaller numbers are 'good', so that it matches the number of records layer that's also outputted by this function
      ungroup()
    
    # convert to a spatial points data frame to match with raster
    xy <- record_weights[,c("lon","lat")]
    spdf.recs_weight <- SpatialPointsDataFrame(coords = xy, data = record_weights,
                                               proj4string = CRS(paste0("+init=epsg:", Coords))) 
    
    ### create counts raster to store number of records in each cell ###
    # this is used to make an 'effort' layer
    n_recs <- template_raster
    
    # make a raster of zeroes for input
    n_recs[!is.na(n_recs)] <- 0
    
    # get cell numbers for the spdf.recs data frame
    cells  <- (cellFromXY(n_recs,spdf.recs_weight))
    
    # fill those cells with the score, have done some tests and 98% sure they are in the same order but ideally would need a match statement here
    n_recs[cells] <- spdf.recs_weight$score
    
  } else if(!weight_by_time){
    
    # convert to a spatial points data frame to match with raster
    xy <- record_counts[,c("lon","lat")]
    spdf.recs <- SpatialPointsDataFrame(coords = xy, data = record_counts,
                                        proj4string = CRS(paste0("+init=epsg:", Coords))) 
    
    ### create counts raster to store number of records in each cell ###
    # this is used to make an 'effort' layer
    n_recs <- template_raster
    
    # make a raster of zeroes for input
    n_recs[!is.na(n_recs)] <- 0
    
    # get the cell index for each point and make a table:
    counts  <- table(cellFromXY(n_recs,spdf.recs))
    
    # fill in the raster with the counts from the cell index:
    n_recs[as.numeric(names(counts))] <- counts
    
  } else {stop('Weight by time mst be TRUE/FALSE')}
  
  return(n_recs)
  
}



```


In both plots, larger numbers are bad (i.e. most/most recent records) and smaller numbers are better (i.e. fewest/oldest records). I have shown how both methods work in the rest of the workflow (i.e. two sets of plots for the next functions). 'Number of records' refers to weighting by the raw numbers; 'time since records' refers to weighting by the time since the most recent records.


```{r, fig.width=10}

rec_counts <- count_records(records_df = records, 
                            template_raster = dec_crop,
                            weight_by_time = FALSE)

rec_counts_time <- count_records(records_df = records, 
                                 template_raster = dec_crop,
                                 weight_by_time = TRUE)
par(mfrow = c(1,2))
plot(rec_counts, main = 'number of records')
plot(rec_counts_time, main = 'time since records')
par(mfrow = c(1,1))

```

## function to create a smoothed recording layer

The function below uses a moving window to sum all the records within a 5x5 matrix to create a 'smoothed' effort layer. It then does:

$1/(1+(SmoothedEffort*RecordingImpact))$

Where recording impact is a way to inflate or reduce the effect of the effort layer. The "1+" is because lots of values were between 0-1 which caused problems when dividing the 1 by the number. This means that high numbers of $(SmoothedEffort*RecordingImpact)$ ends up in a very low weighting score for a grid cell. The raster generated is then simply multiplied by the mean of the raw variation across all species to produce a DECIDE score which has been downweighted by records.

```{r}

smooth_recording <- function(weighted_layer, # layer to be weighted
                             effort_raster, 
                             recording_impact = 10) # the larger the value the less impact it has
{
  
  smoothed_effort <- focal(x = effort_raster, 
                           w = matrix(c(rep(0.1, 5),
                                        0.1, 0.3, 0.3, 0.3, 0.1,
                                        0.1, 0.3,   1, 0.3, 0.1,
                                        0.1, 0.3, 0.3, 0.3, 0.1,
                                        rep(0.1, 5)), 
                                      nrow = 5, ncol = 5),
                           pad = TRUE,
                           padValue = 0,
                           NAonly=T)
  
  # Convert recording to weighting
  weighting <- smoothed_effort
  weighting <- 1/(1+(weighting*recording_impact)) # Outputs a layer of 1s where there are no records and progressively goes to 0 where there are more records
  
  # set minimum value to original layer
  M <- minValue(weighted_layer)
  
  # for now, just multiplying the weighted_layer by the weighting
  adjusted_score <- weighted_layer * weighting
  adjusted_score[adjusted_score < M] <- M # change the lowest records to be equal to the original decide score minimum
  
  
  
  return(list(weighting_layer = weighting,
              weighted_score = adjusted_score))
  
}


```


```{r, fig.width=10}

sr <- smooth_recording(weighted_layer = dec_crop,
                       effort_raster = rec_counts,
                       recording_impact = 5)

sr_time <- smooth_recording(weighted_layer = dec_crop,
                            effort_raster = rec_counts_time,
                            recording_impact = 5)

par(mfrow = c(2,3))
plot(dec_crop, main = 'Raw DECIDE variation')
plot(sr$weighting_layer, main = 'weight layer, number of records')
plot(sr$weighted_score, main = 'Downweighted by records')

plot(dec_crop, main = 'Raw DECIDE variation')
plot(sr_time$weighting_layer, main = 'weight layer, 1/(number of years since last record)')
plot(sr_time$weighted_score, main = 'Downweighted by timing of records')
par(mfrow = c(1,1))

```



