---
title: "DECIDE Score"
author: "Thomas MM"
date: "4/8/2021"
output: html_document
---



The point of this document is to explore some potential options for creating a DECIDE score across all moth species. All plots are centred around Wallingford with a 5k radius. Models aren't the most up-to-date either, they're currently using data with replicates in each cell and aren't weighted by AUC values when combined.


```{r setup, include=F, echo = FALSE}


knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning=FALSE,
                      include = T)

library(spatstat)
library(tidyverse)
library(raster)
library(foreach)
library(doParallel)


source("../modules/filter_distance.R")
source("../modules/recommend_rank.R")
source("../modules/recommend_metric.R")
source("../modules/recommend_agg_rank.R")
source("../modules/extract_metric.R")



options(col=viridis(n = 1e4))

# unweighted geometric mean
gm_mean = function(x, na.rm=FALSE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}

# weighted geometric mean
weighted.geomean <- function(x, w, na.rm = FALSE, ...){
  return(prod(x^w, na.rm = na.rm, ...)^(1/sum(w, na.rm = na.rm, ...)))
}



```


<!-- ## Load species data -->

<!-- This code loads in data from my directory in the object store which is publicly available (as long as your notebook has access to the object store). -->

```{r species_data, echo=F, warning=FALSE, include=F}


model = c('rf', 'lr', 'gam')
taxa = 'moth'

# get a list of all the species that appear in the outputs
# these are files on the object store which is freely accessible to anyone
spp_names_lr <- unique(gsub(pattern="lr_SDMs_|_meanpred.grd|_quantilemaxmin.grd|_quantilerange.grd", replacement = '', 
                            x = list.files(paste0('/data-s3/thoval/sdm_outputs/', taxa, '/lr'), pattern = '.grd')))

spp_names_rf <- unique(gsub(pattern="rf_SDMs_|_meanpred.grd|_quantilemaxmin.grd|_quantilerange.grd", replacement = '', 
                            x = list.files(paste0('/data-s3/thoval/sdm_outputs/', taxa, '/rf'), pattern = '.grd')))

spp_names_gam <- unique(gsub(pattern="gam_SDMs_|_meanpred.grd|_quantilemaxmin.grd|_quantilerange.grd", replacement = '', 
                             x = list.files(paste0('/data-s3/thoval/sdm_outputs/', taxa, '/gam'), pattern = '.grd')))

names <- unique(c(spp_names_lr, spp_names_rf, spp_names_gam))

# sdm outputs for each species
species_stack <- list()

# error outputs
error_out <- list()

for(i in 1:length(names)){
  
  print(names[i])
  
  # initiate model list within for loop so that it gets replaced when starting a new species
  # otherwise we might get some weird overlaps
  model_stack <- list()
  errored_models <- list()
  
  for(m in 1:length(model)){
    
    check_models <- list.files(paste0('/data-s3/thoval/sdm_outputs/', taxa, '/', model[m]), 
                               pattern = paste0(names[i]),
                               full.names = TRUE)
    
    if(length(check_models)<=1){
      
      print(paste('!!!   model', model[m], 'failed for species', names[i], '  !!!'))
      
      errored_models[[m]] <- data.frame(taxa = taxa, 
                                        species = names[i], 
                                        model = model[m])
      
      next
    }
    
    # mean predictions
    mp <- list.files(paste0('/data-s3/thoval/sdm_outputs/', taxa, '/', model[m]), 
                     pattern = paste0(names[i], "_meanpred.grd"),
                     full.names = TRUE)
    
    mod_preds <- raster::stack(mp)
    names(mod_preds) <- paste0(names[i], '_', model[m],'_mean_pred')
    
    
    
    # quantile min/max
    mm <- list.files(paste0('/data-s3/thoval/sdm_outputs/', taxa, '/', model[m]), 
                     pattern = paste0(names[i], "_quantilemaxmin.grd"),
                     full.names = TRUE)
    
    qminmax <- raster::stack(mm)
    names(qminmax) <- c(paste0(names[i], '_', model[m],'_min'), paste0(names[i], '_', model[m],'_max'))
    
    
    # quantile range
    qr <- list.files(paste0('/data-s3/thoval/sdm_outputs/', taxa, '/', model[m]), 
                     pattern = paste0(names[i], "_quantilerange.grd"),
                     full.names = TRUE)
    
    qrange <- raster::stack(qr)
    names(qrange) <- paste0(names[i], '_', model[m], '_quantile_range')
    
    
    # stack all from one model together
    model_stack[[m]] <- raster::stack(mod_preds, qminmax, qrange)
    
  }
  
  # model_stack[sapply(model_stack,is.null)] <- raster(nrow=12500, 
  #                                                    ncol=7000,
  #                                                    crs="+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs")
  
  # To combine them together need to remove the NULL raster layers (i.e. if a model hasn't worked)
  model_stack <- model_stack[!sapply(model_stack,is.null)]
  
  species_stack[[i]] <- raster::stack(model_stack)
  
  # Output the models that failed too
  error_out[[i]] <- do.call('rbind', errored_models) 
  
}

# which models didn't work
errors <- do.call('rbind', error_out)
errors

# name the list entries
names(species_stack) <- names

```


<!-- ## Crop species to smaller scale -->


```{r cropping, echo=F, warning=FALSE, include=F}

# set location 
# location = c(-2.730696, 54.026759) # quernmore
location = c(-1.110557, 51.602436) # wallingford
# location = c(-1.117329, 53.947566) # york

# distances
distance = 5000


registerDoParallel(7)

# out_cropped <- list()
system.time(
  out_cropped <- foreach(s = 1:length(species_stack)) %dopar% {
    
    print(s)
    
    sp <- species_stack[[s]]
    
    
    # crop the prediction
    crop_pred <- filter_distance(obj = subset(sp, grep(pattern = 'mean_pred',
                                                       names(sp))),
                                 method = 'buffer',
                                 distance = distance,
                                 location = location)
    
    # crop the error
    crop_err <- filter_distance(obj = subset(sp, grep(pattern = 'quantile_range',
                                                      names(sp))),
                                method = 'buffer',
                                distance = distance,
                                location = location)
    
    if(length(names(crop_pred))>1){
      # get the mean
      m_pred_av <- calc(crop_pred,
                        mean, na.rm = T)
      names(m_pred_av) <- 'predictions'
      
      
      m_quant_av <- calc(crop_err,
                         mean, na.rm = T)
      names(m_quant_av) <- 'error'
    } else {
      
      m_pred_av <- crop_pred
      m_quant_av <- crop_err
      
    }
    
    out_rasts <- list(m_pred_av, m_quant_av)
    names(out_rasts) <- c('predictions', 'quantile_var')
    
    return(out_rasts)
    
  }
)

registerDoSEQ()

names(out_cropped) <- names

```


## DECIDE Score, addition vs multiply

### Score for a single species

We can combine the probability of presence layers for each species by adding them together or multiplying them. Here is a comparison between the methods for an example species, *Adscita geryon*.


```{r score, echo=F, fig.height=8, fig.width=10, echo=F, warning=FALSE}

score <- recommend_metric(prediction_raster = out_cropped$Adscita_geryon$predictions,
                          error_raster = out_cropped$Adscita_geryon$quantile_var)

par(mfrow = c(2,2))
plot(out_cropped$Adscita_geryon$predictions, main = 'prediction')
plot(out_cropped$Adscita_geryon$quantile_var, main = 'variation')

plot(score$multiply, main = names(score)[1])
plot(score$additive, main = names(score)[2])
par(mfrow = c(1,1))


```


```{r score_mult_spp, echo=F, warning = FALSE}

score_mult <- lapply(c(1:length(out_cropped)), FUN = function(x){
  
  rm <- recommend_metric(prediction_raster = out_cropped[[x]]$predictions,
                         error_raster = out_cropped[[x]]$quantile_var,
                         method = 'multiply')$multiply
  
  return(rm)
  
})

names(score_mult) <- names(out_cropped)


# get the cropped probability of presence
preds <- stack(lapply(1:length(out_cropped), FUN = function(x) out_cropped[[x]]$predictions))
names(preds) <- names(out_cropped)

# get the cropped variation
var <- stack(lapply(1:length(out_cropped), FUN = function(x) out_cropped[[x]]$quantile_var))
names(var) <- names(out_cropped)



```

We decided in a previous meeting that it makes most sense to use the multiplicative method of combining probability of presence and variation layers, so this is what I've used for the rest of the plots.

## How to combine DECIDE score across species?

We have decided to produce a metric which is evaluated across species; this will be easier for any user to digest and does not leave us open to too much criticism in terms of recorders claiming the models are wrong. Ideally, we want to combine them in such a way that:

- rare species (locally, nationally or both[?]) are not downweighted
* solution: weighted by national scarcity?
- common species are maybe downweighted
- habitat specialists are not downweighted compared to generalists
* potential solution: Find only species that have a greater than X prob presence in a region. Find the ones with the fewest cells with their value above a certain (high) threshold?
- priority given to species of conservation concern?

However, we need to make sure that including weightings doesn't affect the adaptive sampling process too much and affect our ability to evaluate whether the app worked at the end of the project. Multiple potential options to test regarding how to combine the multiplied score across species into a 'DECIDE score':

1. take sum across species
2. take the mean across species
3. weighted average based on maximum probability of presence within region
4. weighted average based on maximum probability of presence within larger region (~30km?)
5. weighted average based on national scarcity - number of unique grid cells a species occurs in across the UK
6. weighted average based on max uncertainty within region
7. weighted average based on mean uncertainty within region

Want to compare all these possibilities using both the arithmetic mean and geometric mean to see which looks better. The plots will be numbered by the points listed above. Sorry they're so small, I to be able to compare them all easily. 


```{r 1_comb_arithmetic, warning = F, echo = F}

dfm <- read.csv('../../Data/species_data/moth/DayFlyingMoths_East_Norths.csv')

dfm <- read.csv('../../Data/species_data/moth/DayFlyingMoths_East_Norths.csv')

# get the cropped probability of presence
preds <- stack(lapply(1:length(out_cropped), FUN = function(x) out_cropped[[x]]$predictions))
names(preds) <- names(out_cropped)

# get the cropped variation
var <- stack(lapply(1:length(out_cropped), FUN = function(x) out_cropped[[x]]$quantile_var))
names(var) <- names(out_cropped)


## 1.
sum_1 <- calc(stack(score_mult), sum)
plot(sum_1, main = '1. Sum')

## 2.
mean_2 <- calc(stack(score_mult), mean)
# plot(mean_2, main = '2. arithmetic mean')

## 3. 
arit_weight_max_3 <- weighted.mean(stack(score_mult), cellStats(stack(preds), max)) # the score_mult and preds stacks are in the same species order
# plot(arit_weight_max_3, main = '3. arith_mean weighted by max prob. pres.')

## 4.
# function to get the corresponding layer of interest in a raster stack
# takes a list of stacks and the layer name to use
get_layer <- function(layer_stack, pattern) return(lapply(layer_stack, FUN = function(x) subset(x, grep(pattern = pattern, names(x)))))

preds_uk <- get_layer(species_stack, pattern = 'mean_pred') # get the mean preds layer

# crop the uk-wide predictions to 30km around region
crop_30 <- mcmapply(preds_uk, FUN = function(x) filter_distance(obj = x, method = 'buffer', distance = 30000, location = location))

# just get the mean across all models - because I need to transfer the new AUC-combined models across
preds_30km_mn <- sapply(1:length(crop_30), FUN = function(x) if(nlayers(crop_30[[x]])==1) {crop_30[[x]]} else {calc(crop_30[[x]], mean, na.rm = T)})

arit_weight_max30km_4 <- weighted.mean(stack(score_mult), cellStats(stack(preds_30km_mn), max)) # do the mean weighted by the max probability of presence within 30km
# plot(arit_weight_max30km_4, main = '4. arith_mean weighted by max prob. pres. 30km')

## 5. 
# function to rescale between 0 and 1
range01 <- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}

# first, get scarcity
dfm_prev <- dfm %>% group_by(sp_n) %>% 
  mutate(n_cells = length(unique(TO_GRIDREF))) %>% # number of unique locations
  ungroup() %>% 
  mutate(prev = n_cells/length(unique(TO_GRIDREF))) %>% # get the prevalence of each species relative to the total number of grid cells sampled
  dplyr::select(sp_n, n_cells, prev) %>% # select only columns of interest
  distinct() %>% 
  mutate(sp_n = gsub(pattern = ' ', replacement = '_', x = sp_n))

# dfm_prev

# get weightings first by matching names of list entries to those in data frame
ukprev_wts <- sapply(1:length(score_mult), FUN = function(x) dfm_prev$prev[dfm_prev$sp_n==names(score_mult)[x]])

# 1.000001-ukprev_wts # get the inverse weights to give least prevalent species highest rating. Add a tiny bit so the most abundant species doesn't get a 0-weight

arit_weight_ukprev_5 <- weighted.mean(stack(score_mult), 1.000001-ukprev_wts)
# plot(arit_weight_ukprev_5, main = '5. arith_mean weighted by national scarcity')

## 6.
arit_weight_maxvar_6 <- weighted.mean(stack(score_mult), cellStats(stack(var), max)) # the score_mult and var stacks are in the same species order
# plot(arit_weight_maxvar_6)

## 7.
arit_weight_meanvar_7 <- weighted.mean(stack(score_mult), cellStats(stack(var), mean)) # the score_mult and var stacks are in the same species order
# plot(arit_weight_meanvar_7)

```




```{r geo_mean, warning = FALSE, echo = F}

## 1.
sum_1 <- calc(stack(score_mult), sum)
# plot(sum_1, main = '1. Sum')

## 2. 
geo_mean_2 <- calc(stack(score_mult), gm_mean)
# plot(geo_mean_2, main = '2. geometric mean')

## 3. 
geo_weight_max_3 <- weighted.geomean(stack(score_mult), cellStats(stack(preds), max))
# plot(geo_weight_max_3, main = '3. geo_mean weighted by max prob. pres.')

## 4.
geo_weight_max30km_4 <- weighted.geomean(stack(score_mult), cellStats(stack(preds_30km_mn), max)) # do the mean weighted by the max probability of presence within 30km
# plot(geo_weight_max30km_4, main = '4. geo_mean weighted by max prob. pres. 30km')

## 5. 
# same weightings as above
# calculate weighted geometric mean
geo_weight_ukprev_5 <- weighted.geomean(stack(score_mult), 1.000001-ukprev_wts)
# plot(geo_weight_ukprev_5, main = '5. geo_mean weighted by national scarcity')

## 6.
geo_weight_maxvar_6 <- weighted.geomean(stack(score_mult), cellStats(stack(var), max)) # the score_mult and var stacks are in the same species order
# plot(geo_weight_maxvar_6)

## 7.
geo_weight_meanvar_7 <- weighted.geomean(stack(score_mult), cellStats(stack(var), mean)) # the score_mult and var stacks are in the same species order
# plot(geo_weight_meanvar_7)

```


```{r all_ps, fig.height = 7, fig.width=20, meassage = F, warning=F, echo = F}

par(mfrow =c(2,6))

plot(mean_2, main = '2. arithmetic mean')
plot(arit_weight_max_3, main = '3. arith_mean, wt max prob. pres.')
plot(arit_weight_max30km_4, main = '4. arith_mean, wt max prob. pres. 30km')
plot(arit_weight_ukprev_5, main = '5. arith_mean, wt national scarcity')
plot(arit_weight_maxvar_6, main = '6. arit_mean, wt max var.')
plot(arit_weight_meanvar_7, main = '6. arit_mean, wt mean var.')


plot(geo_mean_2, main = '2. geometric mean')
plot(geo_weight_max_3, main = '3. geo_mean, wt max prob. pres.')
plot(geo_weight_max30km_4, main = '4. geo_mean, wt max prob. pres. 30km')
plot(geo_weight_ukprev_5, main = '5. geo_mean, wt national scarcity')
plot(geo_weight_maxvar_6, main = '6. geo_mean, wt max var.')
plot(geo_weight_meanvar_7, main = '6. geo_mean, wt mean var.')

par(mfrow=c(1,1))

```

Taking the weighted geometric mean results in some areas having a 0 value, which would be very bad to have in the final app. To stop this can add a tiny constant to the values before multiplying them...

#### Geometric mean with a constant added constant

The top row of plots are the geometric means without the constant added, the bottom row has the constant added as well. I think the plots with the constant added look a little bit better.

```{r geo_mean_const, warning = FALSE, echo = F}

constant <- 1e-5

## 1.
sum_1 <- calc(stack(score_mult), sum)
# plot(sum_1, main = '1. Sum')

## 2. 
geo_mean_2_cons <- calc(stack(score_mult)+constant, gm_mean)
# plot(geo_mean_2, main = '2. geometric mean')

## 3. 
geo_weight_max_3_cons <- weighted.geomean(stack(score_mult)+constant, cellStats(stack(preds), max)+constant)
# plot(geo_weight_max_3, main = '3. geo_mean weighted by max prob. pres.')

## 4.
geo_weight_max30km_4_cons <- weighted.geomean(stack(score_mult)+constant, cellStats(stack(preds_30km_mn), max)+constant) # do the mean weighted by the max probability of presence within 30km
# plot(geo_weight_max30km_4, main = '4. geo_mean weighted by max prob. pres. 30km')

## 5. 
# same weightings as above
# calculate weighted geometric mean
geo_weight_ukprev_5_cons <- weighted.geomean(stack(score_mult)+constant, 1.000001-ukprev_wts+constant)
# plot(geo_weight_ukprev_5, main = '5. geo_mean weighted by national scarcity')

## 6.
geo_weight_maxvar_6_cons <- weighted.geomean(stack(score_mult)+constant, cellStats(stack(var), max)+constant) # the score_mult and var stacks are in the same species order
# plot(geo_weight_maxvar_6)

## 7.
geo_weight_meanvar_7_cons <- weighted.geomean(stack(score_mult)+constant, cellStats(stack(var), mean)+constant) # the score_mult and var stacks are in the same species order
# plot(geo_weight_meanvar_7)

```


```{r all_ps_const, fig.height = 7, fig.width=20, meassage = F, warning=F, echo = F}

par(mfrow =c(2,6))

plot(geo_mean_2, main = '2. geometric mean')
plot(geo_weight_max_3, main = '3. geo_mean, wt max prob. pres.')
plot(geo_weight_max30km_4, main = '4. geo_mean, wt max prob. pres. 30km')
plot(geo_weight_ukprev_5, main = '5. geo_mean, wt national scarcity')
plot(geo_weight_maxvar_6, main = '6. geo_mean, wt max var.')
plot(geo_weight_meanvar_7, main = '6. geo_mean, wt mean var.')

plot(geo_mean_2_cons, main = '2. geometric mean + constant')
plot(geo_weight_max_3_cons, main = '3. geo_mean, wt max prob. pres. + constant')
plot(geo_weight_max30km_4_cons, main = '4. geo_mean, wt max prob. pres. 30km + constant')
plot(geo_weight_ukprev_5_cons, main = '5. geo_mean, wt national scarcity + constant')
plot(geo_weight_maxvar_6_cons, main = '6. geo_mean, wt max var. + constant')
plot(geo_weight_meanvar_7_cons, main = '6. geo_mean, wt mean var. + constant')

par(mfrow=c(1,1))

```

The last two big multi-plots are the main point of this document, for you to have a look at the outputs of different ways of creating a DECIDE score. Below are a few thoughts that I've had while doing this, which may or may not be of interest. 

#### Just a couple of thoughts on the different options

1. take sum across species

Summing the DECIDE scores across species probably isn't very meaningful, but just wanted to show what it looked like.

2. take the mean across species

This might actually just be the easiest way for this first iteration. It's easy to explain and understand and might be a decent enough place-holder until we get some MCDM processes up and running.

3. weighted average based on maximum probability of presence within region
4. weighted average based on maximum probability of presence within larger region (~30km?)

Weighted means using the probability of presence within the area means the DECIDE score is specific to that area. This means the DECIDE score wouldn't be comparable across the whole of the UK, which could be a problem when evaluating the project? Weighting the score by the max probability of presence means that species that are most likely to be present in an area will be upweighted compared to species that are less likely. This might not be the best idea because it means that common species are likely to be upweighted across the whole of the UK. I.e. common species are likely to have high maximum probabilities of presence across much of the UK. A slight alteration of this might be a good way to increase the importance of habitat specialists: we could account for number of grid cells within a specific area with a probability of presence higher than some threshold, rather than using the maximum.  

5. weighted average based on national scarcity - number of unique grid cells a species occurs in across the UK

'National scarcity' is calculated by looking at the unique number of grid cells that a species appears in and dividing it by the total number of unique grid cells that have been sampled (rather than the total number of grid cells in the UK). However, this might not be sensible because this could, in some situations, upweight species that are extremely unlikely to be present in an area. For example, a species that only occurs in the Scottish Highlands would be very scarce nationally and would never be found in southern England. However, because all species have a probability of presence in every grid cell in the UK, that species would get strongly upweighted in the DECIDE score.

6. weighted average based on maximum uncertainty within region
7. weighted average based on mean uncertainty within region

If we are interested in improving the models the most, this will probably be by sending people to areas with the highest uncertainty. Weighting the score by the maximum uncertainty within a region might be a really good way of improving the models. As you can see in the figure below, the relationship between the probability of presence and variation layers tends to be quadratic, which means that cells with mid-levels of predicted probabilities would be upweighted. This will of course influence how likely a recorder is to see a species, which might be a problem from a user's perspective. Caveat, I don't yet know what's going on with *Siona lineata*'s 1:1 relationship in the figure below.


```{r prb_var_rel, echo=FALSE, message = F, warning = F}

pred_df <- stack(preds) %>% as.data.frame(xy=T) %>% 
  pivot_longer(cols = c(3:44), names_to = 'species_pred', values_to = 'preds')
var_df <- stack(var) %>% as.data.frame(xy=T) %>% 
  pivot_longer(cols = c(3:44), names_to = 'species_var', values_to = 'var') %>% 
  rename(xv = x, yv = y)


check <- (cbind(pred_df, var_df)) %>% 
  na.omit() %>% 
  mutate(check = ifelse(paste0(x,y,species_pred)!=paste0(xv,yv,species_var),1,0))

# head(check)
# 
# sum(check$check) ## all good

check %>% 
  ggplot() +
  # geom_point(aes(x = preds, y = var, colour = species_pred), show.legend = FALSE, alpha = 0.2) +
  geom_smooth(aes(x = preds, y = var, colour = species_pred), show.legend = FALSE) +
  theme_classic() +
  labs(caption = 'Relationship between variation and probability of presence, coloured by species',
       x = 'Predicted probability',
       y = 'Variation') +
  theme(plot.caption = element_text(hjust = 0, size = 12))

par(mfrow = c(1,2))
plot(stack(preds)[[33]], main = paste(names(stack(preds))[33], 'predictions'))
plot(stack(var)[[33]], main = paste(names(stack(preds))[33], 'variation'))
par(mfrow=c(1,1))

```


#### General points

Does it make sense to weight the average by a component of the score itself, i.e., the score is probability of presence*var and the mean across species is weighted by the max probability of presence? I guess they aren't technically the same thing but might this be a problem?

Is it better just to take the mean across all species, without any weightings? This means that each species will be given the same weighting in the final value of a cell, so that we're not trying to alter the data in a way that 'suits' us. If we take the geometric mean it reduces the influence of outliers which could be good. Conversely, it might be better to have the DECIDE score be strongly influenced by outliers so that cells containing particularly high uncertainty or probability of presence are favoured, in which case, the arithmetic mean might be better. 

What about combining species by taking the weighted average of the probability of presence layers and using the quantile variation layer as a weight? I.e. each cell would be weighted by its corresponding variation in the same cell...
* means we can't assess/don't have the decide score of each species?

Also, the arithmetic mean of all DECIDE scores is very uniform across the whole area. Tom suggested plotting using the square of the arithmetic mean to 'spread out' the values a little more.


```{r extra_plots, fig.height = 5, fig.width=20, include = T, echo=T}


wt_decide <- raster::weighted.mean(x = preds, 
                                   w = var)
par(mfrow = c(1,2))
plot(wt_decide, main = 'arith mean, prob weighted by uncertainty')

geowt_decide <- weighted.geomean(x = preds,
                                 w = var)

plot(geowt_decide, main = 'geo mean, prob weighted by uncertainty')


plot(mean_2, main = '2. arithmetic mean')
plot(mean_2^2, main = '2. arithmetic mean squared')

par(mfrow =c(1,6))

plot(mean_2^2, main = '2. arithmetic mean squared')
plot(arit_weight_max_3^2, main = '3. arith_mean, wt max prob. pres. squared')
plot(arit_weight_max30km_4^2, main = '4. arith_mean, wt max prob. pres. 30km squared')
plot(arit_weight_ukprev_5^2, main = '5. arith_mean, wt national scarcity squared')
plot(arit_weight_maxvar_6^2, main = '6. arit_mean, wt max var. squared')
plot(arit_weight_meanvar_7^2, main = '6. arit_mean, wt mean var. squared')

par(mfrow=c(1,1))



```







